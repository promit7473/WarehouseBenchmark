"""
Benchmarking Suite Module

This module provides comprehensive benchmarking and analysis tools
for warehouse RL experiments, inspired by RLRoverLab's evaluation framework.
"""

from dataclasses import dataclass
from typing import Dict, List, Any, Optional
import json
import numpy as np
from pathlib import Path


@dataclass
class BenchmarkResult:
    """Container for benchmark results."""
    algorithm: str
    environment: str
    episode_rewards: List[float]
    episode_lengths: List[float]
    success_rates: List[float]
    collision_rates: List[float]
    training_time: float
    final_score: float
    metadata: Dict[str, Any]


class WarehouseBenchmarkSuite:
    """Comprehensive benchmarking suite for warehouse RL algorithms."""

    def __init__(self, results_dir: str = "benchmark_results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        self.results: List[BenchmarkResult] = []

    def run_comprehensive_benchmark(
        self,
        algorithms: List[str],
        environments: List[str],
        num_runs: int = 3,
        max_episodes: int = 1000
    ) -> Dict[str, Any]:
        """Run comprehensive benchmark across algorithms and environments."""
        print("ðŸš€ Starting Comprehensive Warehouse Benchmark")

        all_results = {}
        for env in environments:
            for alg in algorithms:
                print(f"ðŸ­ Testing {alg} on {env}")
                # Simulate results for now
                results = [self._simulate_training_run(alg, env, max_episodes) for _ in range(num_runs)]
                analysis = self._analyze_algorithm_results(alg, env, results)
                all_results[f"{env}_{alg}"] = analysis
                self._save_results(alg, env, results, analysis)

        return self._generate_final_report(all_results)

    def _simulate_training_run(self, algorithm: str, environment: str, max_episodes: int) -> BenchmarkResult:
        """Simulate a training run."""
        final_score = {"PPO": 0.85, "SAC": 0.82, "TD3": 0.78, "DQN": 0.65}.get(algorithm, 0.7)
        episode_rewards = np.clip(np.cumsum(np.random.normal(0.01, 0.1, max_episodes)), 0, 1).tolist()

        return BenchmarkResult(
            algorithm=algorithm,
            environment=environment,
            episode_rewards=episode_rewards,
            episode_lengths=[150] * len(episode_rewards),
            success_rates=[min(1.0, r / 2.0) for r in episode_rewards],
            collision_rates=[max(0.0, 0.1 - r / 10.0) for r in episode_rewards],
            training_time=max_episodes * 0.1,
            final_score=final_score,
            metadata={"convergence_episode": int(max_episodes * 0.7)}
        )

    def _analyze_algorithm_results(self, algorithm: str, environment: str, results: List[BenchmarkResult]) -> Dict[str, Any]:
        """Analyze results for an algorithm-environment pair."""
        final_scores = [r.final_score for r in results]
        return {
            "algorithm": algorithm,
            "environment": environment,
            "mean_final_score": np.mean(final_scores),
            "std_final_score": np.std(final_scores),
            "performance_rank": 1  # Placeholder
        }

    def _save_results(self, algorithm: str, environment: str, results: List[BenchmarkResult], analysis: Dict[str, Any]):
        """Save results."""
        save_dir = self.results_dir / f"{environment}_{algorithm}"
        save_dir.mkdir(exist_ok=True)

        results_data = {"analysis": analysis, "runs": len(results)}
        with open(save_dir / "results.json", 'w') as f:
            json.dump(results_data, f, indent=2)

    def _generate_final_report(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate final report."""
        return {
            "summary": {
                "total_experiments": len(all_results),
                "best_algorithm": max(all_results.keys(), key=lambda x: all_results[x]["mean_final_score"])
            },
            "detailed_results": all_results
        }


# Convenience functions
def run_quick_benchmark(algorithms: List[str] = ["PPO", "SAC", "TD3"],
                       environments: List[str] = ["BasicWarehouse", "WarehouseWithObstacles"],
                       num_runs: int = 2) -> Dict[str, Any]:
    """Run a quick benchmark."""
    suite = WarehouseBenchmarkSuite("quick_benchmark_results")
    return suite.run_comprehensive_benchmark(algorithms, environments, num_runs, 500)


__all__ = [
    "WarehouseBenchmarkSuite",
    "BenchmarkResult",
    "run_quick_benchmark",
]